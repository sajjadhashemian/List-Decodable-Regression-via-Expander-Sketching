# Algorithmic Details

This document describes how Algorithm 1 “Expander–Sketched List-Decodable Regression” is implemented in this repository.

We decompose the pipeline into five modules:

1. **Expander sketch construction** (`expander.py`)
2. **Bucket-wise normal equations** (`bucket_stats.py`)
3. **Robust aggregation** (`robust_agg.py`)
4. **Spectral filtering** (`filtering.py`)
5. **List generation / clustering** (`clustering.py`)

---

## 1. Expander sketch

We represent each repetition \(t \in \{1,\dots,r\}\) by:

- A mapping `bucket_indices[t][b]` containing the indices of samples mapped to bucket \(b\),
- A matching array of signs `bucket_signs[t][b]` in \(\{\pm 1\}\).

These are generated by sampling, for each sample \(i\):

- `left_degree` distinct buckets uniformly from `0, …, B-1`,
- independent Rademacher signs.

This random construction is known to be a lossless expander with high probability for suitable parameters. The theoretical algorithm uses explicit or random lossless expanders; we implement the random version.

`ExpanderSketcher` provides:

- `fit(n_samples)` – generate the expander structure,
- `assign_indices()` – return bucket → indices and signs.

---

## 2. Bucket-wise normal equations

Given features `X` (shape `(n, d)`), labels `y` (shape `(n,)`), and bucket assignments, we define for each active bucket \((t, b)\):

- Signed inlier design matrix: \(A_{t,b}\) with rows `sign * x_i`,
- Signed response vector: \(b_{t,b}\) with entries `sign * y_i`.

Local normal equations:

\[
H_{t,b} = \frac{1}{|I_{t,b}|}\sum_{i \in I_{t,b}} x_i x_i^\top, \quad
 g_{t,b} = \frac{1}{|I_{t,b}|}\sum_{i \in I_{t,b}} x_i y_i.
\]

`BucketStatistics` computes:

- `compute_moments(active_buckets)` → list of `(H, g)` pairs,
- `compute_residual_covariances(l_hat, active_buckets)` → per-bucket residual covariance matrices
  \[
  C_{t,b} = A_{t,b}^\top \operatorname{diag}(r_{t,b}^2) A_{t,b},
  \quad r_{t,b} = b_{t,b} - A_{t,b}\,\hat\ell.
  \]

---

## 3. Robust aggregation

We collect all local statistics into a set
\[
S = \{(H_{t,b}, g_{t,b}) : (t,b) \in \mathcal{B}\},
\]
where \(\mathcal{B}\) is the set of active buckets.

We partition `S` into `M` blocks `S_m` of nearly equal size:

- Block means: \(H_m = |S_m|^{-1}\sum_{(H,g)\in S_m} H\), \(g_m = |S_m|^{-1}\sum_{(H,g)\in S_m} g\).

We then robustly aggregate \(\{H_m\}_{m=1}^M\) and \(\{g_m\}_{m=1}^M\) using either:

- **Median-of-means** in each coordinate, or
- **Geometric median** in Euclidean / Frobenius norm.

The implementation defaults to geometric median for matrices (`H_m`) and vectors (`g_m`).

Result: global moment estimates \(\hat\Sigma\) and \(\hat g\).

---

## 4. Spectral filtering

Given \((\hat\Sigma, \hat g)\), we solve the sketched normal equations:

\[
(\hat\Sigma + \lambda I)\,\hat\ell = \hat g
\]

for some ridge parameter \(\lambda \ge 0\).

We then iterate:

1. Compute residuals per bucket:
   \[
   r_{t,b} = b_{t,b} - A_{t,b}\,\hat\ell.
   \]

2. Compute bucket-level residual covariance matrices:
   \[
   C_{t,b} = A_{t,b}^\top \operatorname{diag}(r_{t,b}^2)A_{t,b}.
   \]

3. Robustly aggregate \(\{C_{t,b}\}_{(t,b)\in\mathcal{B}}\) to obtain \(\hat C\).

4. Compute the top eigenpair \((\lambda_{\max}, v)\) of \(\hat C\).

5. If \(\lambda_{\max} \le (1 + \eta)\cdot \text{TargetVar}\) (an estimate of the typical inlier variance), stop filtering.

6. Otherwise:
   - Score each bucket by \(s_{t,b} = v^\top C_{t,b} v\),
   - Remove the top \(\rho\) fraction of buckets in \(\mathcal{B}\) by this score,
   - Recompute moments and repeat up to `filtering_rounds` iterations.

This is implemented in `filtering.py` as a `FilteringLoop` class.

---

## 5. List generation and clustering

For each seed \(s = 1, \dots, R\) we run the full pipeline and obtain a candidate \(\hat\ell^{(s)}\).

We then cluster the candidates using **single-linkage clustering** with a distance threshold \(\Delta\) (target accuracy radius). Implementation uses `sklearn.cluster.AgglomerativeClustering` with `linkage="single"` and a distance threshold.

The cluster centers (e.g. medoids) form the final list of regressors.

See `clustering.py` for details.
